{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using inference dtype: torch.float32\n",
      "\n",
      "--- Loading Models: Epoch 10 from './training_diffusion/' ---\n",
      "[Load] Initializing VAE from runwayml/stable-diffusion-v1-5...\n",
      "[Load] Loading custom VAE weights from: /Users/chinmaysultanpuri/Desktop/GenAI/models/vae_full_finetuned_stage2_v10_api_fix_corrected.pth\n",
      "[Load] Custom VAE weights applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] VAE Loaded to mps (torch.float32).\n",
      "\n",
      "[Load] Initializing UNet from runwayml/stable-diffusion-v1-5...\n",
      "[Load] Base UNet loaded. Cross Attn Dim: 768\n",
      "[Load] Loading UNet weights from: /Users/chinmaysultanpuri/Desktop/GenAI/training_diffusion/unet_epoch_10.pth\n",
      "[Load] UNet Loaded to mps (torch.float32).\n",
      "\n",
      "[Load] Initializing ActionConditioner...\n",
      "[ActionConditioner] Initializing with CLIP: openai/clip-vit-large-patch14\n",
      "[ActionConditioner]   CLIP output dim: 768\n",
      "[ActionConditioner]   Freezing CLIP text model parameters.\n",
      "[ActionConditioner]   Added projection layers (target dim 768 might update)\n",
      "[Load] Loading ActionConditioner weights from: /Users/chinmaysultanpuri/Desktop/GenAI/training_diffusion/action_conditioner_epoch_10.pth\n",
      "[Load] ActionConditioner Loaded to mps (torch.float32).\n",
      "\n",
      "--- Model Loading Complete ---\n",
      "\n",
      "--- Initializing Start Latent State (Zeros) ---\n",
      "Initial latent created with shape: torch.Size([1, 4, 64, 64])\n",
      "\n",
      "--- Starting Interactive Generation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab17be62ceb42c39c356d541502cf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Frames:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Step 1/100\n",
      "Empty action entered, stopping generation.\n",
      "\n",
      "--- Inference Loop Finished ---\n",
      "No frames were generated.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#                            Imports\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm # Use notebook version for better display\n",
    "\n",
    "# Diffusers & Transformers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "# from diffusers.schedulers import DDPMScheduler # Not strictly needed for this inference logic\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Disable noisy warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' # Suppress TensorFlow oneDNN warnings if TF is installed\n",
    "\n",
    "# ==============================================================================\n",
    "#                      Configuration (MUST MATCH TRAINING)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Core Model & Paths ---\n",
    "# <<< UPDATE >>> Base model used during training\n",
    "SD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "# <<< UPDATE >>> CLIP model used for actions during training\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-large-patch14\"\n",
    "# <<< UPDATE >>> Directory where training checkpoints are saved\n",
    "CHECKPOINT_DIR = \"./training_diffusion/\"\n",
    "# <<< UPDATE >>> The epoch number of the checkpoint you want to load\n",
    "TARGET_EPOCH = 10\n",
    "# <<< UPDATE (Optional) >>> Path to fine-tuned VAE weights (same as training). Set to None if not used.\n",
    "VAE_PATH = \"./models/vae_full_finetuned_stage2_v10_api_fix_corrected.pth\"\n",
    "\n",
    "# --- Image & Latent Dimensions (Must match training) ---\n",
    "IMAGE_RESOLUTION = 512\n",
    "VAE_SCALE_FACTOR = 0.18215\n",
    "LATENT_CHANNELS = 4 # Should match UNet in_channels\n",
    "# Calculate latent dimensions (common for SD VAE)\n",
    "VAE_DOWNSAMPLE_FACTOR = 8 # Standard for SD VAE\n",
    "LATENT_HEIGHT = IMAGE_RESOLUTION // VAE_DOWNSAMPLE_FACTOR\n",
    "LATENT_WIDTH = IMAGE_RESOLUTION // VAE_DOWNSAMPLE_FACTOR\n",
    "\n",
    "\n",
    "# --- Action Conditioning Module Config (Must match training) ---\n",
    "CLIP_MAX_LENGTH = 16 # Max sequence length for CLIP tokenizer\n",
    "# <<< UPDATE >>> Set based on your training's FREEZE_CLIP_BACKBONE config\n",
    "FREEZE_CLIP_IN_ACTION_MODULE = True\n",
    "\n",
    "# --- Inference Specific Config ---\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# Use float16 for faster inference on GPU, change to torch.float32 if numerical issues occur\n",
    "INFERENCE_DTYPE = torch.float32\n",
    "# <<< UPDATE >>> How many steps/frames to generate in sequence\n",
    "NUM_INFERENCE_STEPS = 100\n",
    "# START_IMAGE_PATH is no longer needed\n",
    "\n",
    "# ==============================================================================\n",
    "#              Model Definitions (Copy EXACTLY from training)\n",
    "# ==============================================================================\n",
    "\n",
    "class ActionConditioningModule(nn.Module):\n",
    "    \"\"\" Encodes action strings using CLIP and projects to target dimension. \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_model_name: str = CLIP_MODEL_NAME,\n",
    "        max_clip_length: int = CLIP_MAX_LENGTH,\n",
    "        target_token_dim: int = 768, # This will be updated based on UNet later\n",
    "        freeze_clip: bool = FREEZE_CLIP_IN_ACTION_MODULE\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.target_token_dim = target_token_dim\n",
    "        self.max_clip_length = max_clip_length\n",
    "        self.freeze_clip = freeze_clip\n",
    "        print(f\"[ActionConditioner] Initializing with CLIP: {clip_model_name}\")\n",
    "        self.clip_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.clip_text_model = CLIPTextModel.from_pretrained(clip_model_name)\n",
    "        clip_output_dim = self.clip_text_model.config.hidden_size\n",
    "        print(f\"[ActionConditioner]   CLIP output dim: {clip_output_dim}\")\n",
    "\n",
    "        if freeze_clip:\n",
    "            print(\"[ActionConditioner]   Freezing CLIP text model parameters.\")\n",
    "            for param in self.clip_text_model.parameters(): param.requires_grad = False\n",
    "            self.clip_text_model.eval() # Ensure it's in eval mode if frozen\n",
    "        else:\n",
    "            print(\"[ActionConditioner]   CLIP text model parameters are TRAINABLE (will be set to eval).\")\n",
    "            # We still set to eval for inference regardless\n",
    "\n",
    "        # Projection layers\n",
    "        self.action_projection1 = nn.Linear(clip_output_dim, clip_output_dim)\n",
    "        self.action_projection2 = nn.Linear(clip_output_dim, target_token_dim) # Target dim might change\n",
    "        print(f\"[ActionConditioner]   Added projection layers (target dim {target_token_dim} might update)\")\n",
    "\n",
    "    def forward(self, action_strings, device):\n",
    "        b = len(action_strings)\n",
    "        clip_inputs = self.clip_tokenizer(\n",
    "            action_strings, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_clip_length, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Ensure eval mode and no gradients during inference\n",
    "        self.clip_text_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.clip_text_model(**clip_inputs)\n",
    "            last_hidden = outputs.last_hidden_state # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "            # --- Using pooled output (like training) ---\n",
    "            eos_token_id = self.clip_tokenizer.eos_token_id\n",
    "            input_ids = clip_inputs[\"input_ids\"]\n",
    "            batch_size, seq_len = last_hidden.shape[0], last_hidden.shape[1]\n",
    "            eos_mask = (input_ids == eos_token_id)\n",
    "            eos_indices = torch.where(\n",
    "                eos_mask.any(dim=1),\n",
    "                eos_mask.max(dim=1).indices,\n",
    "                torch.tensor(seq_len - 1, device=device)\n",
    "            )\n",
    "            eos_indices = eos_indices.clamp(0, seq_len - 1)\n",
    "            pooled = last_hidden[torch.arange(batch_size, device=device), eos_indices]\n",
    "            # --- End Pooled Output ---\n",
    "\n",
    "        # Projection layers (still no grad)\n",
    "        with torch.no_grad():\n",
    "            x = F.silu(self.action_projection1(pooled))\n",
    "            x = self.action_projection2(x)\n",
    "\n",
    "        # Return shape [batch_size, 1, target_token_dim] for UNet cross-attention\n",
    "        return x.unsqueeze(1)\n",
    "\n",
    "# ==============================================================================\n",
    "#                        Loading Function (Corrected)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_trained_models(checkpoint_dir, epoch, sd_model_id, vae_path=None):\n",
    "    \"\"\"Loads the trained UNet, ActionConditioner, and VAE.\"\"\"\n",
    "    print(f\"\\n--- Loading Models: Epoch {epoch} from '{checkpoint_dir}' ---\")\n",
    "\n",
    "    # --- 1. Load VAE ---\n",
    "    print(f\"[Load] Initializing VAE from {sd_model_id}...\")\n",
    "    try:\n",
    "        vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder=\"vae\")\n",
    "        if vae_path and os.path.exists(vae_path):\n",
    "            abs_vae_path = os.path.abspath(vae_path)\n",
    "            print(f\"[Load] Loading custom VAE weights from: {abs_vae_path}\")\n",
    "            if os.path.exists(abs_vae_path):\n",
    "                 vae.load_state_dict(torch.load(abs_vae_path, map_location='cpu'), strict=False)\n",
    "                 print(\"[Load] Custom VAE weights applied.\")\n",
    "            else:\n",
    "                 print(f\"[Load Warning] Custom VAE path specified but not found at {abs_vae_path}. Using base VAE.\")\n",
    "        elif vae_path:\n",
    "             print(f\"[Load Warning] Custom VAE path '{vae_path}' specified but does not exist. Using base VAE.\")\n",
    "        else:\n",
    "             print(\"[Load] Using base VAE weights.\")\n",
    "\n",
    "        vae = vae.to(DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "        vae.eval()\n",
    "        print(f\"[Load] VAE Loaded to {DEVICE} ({INFERENCE_DTYPE}).\") # Corrected print format\n",
    "    except Exception as e:\n",
    "        print(f\"[Load Error] Failed to load VAE: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- 2. Load UNet ---\n",
    "    print(f\"\\n[Load] Initializing UNet from {sd_model_id}...\")\n",
    "    try:\n",
    "        unet = UNet2DConditionModel.from_pretrained(sd_model_id, subfolder=\"unet\")\n",
    "        unet_cross_attention_dim = unet.config.cross_attention_dim\n",
    "        print(f\"[Load] Base UNet loaded. Cross Attn Dim: {unet_cross_attention_dim}\")\n",
    "\n",
    "        # Construct the correct checkpoint filename\n",
    "        unet_filename = f\"unet_epoch_{epoch}.pth\"\n",
    "        unet_path = os.path.join(checkpoint_dir, unet_filename)\n",
    "        unet_path_abs = os.path.abspath(unet_path)\n",
    "\n",
    "        if not os.path.exists(unet_path_abs):\n",
    "            raise FileNotFoundError(f\"UNet checkpoint not found at expected path: {unet_path_abs}\")\n",
    "\n",
    "        print(f\"[Load] Loading UNet weights from: {unet_path_abs}\")\n",
    "        unet_state_dict = torch.load(unet_path_abs, map_location='cpu')\n",
    "\n",
    "        # Handle potential 'module.' prefix from DDP saving\n",
    "        if all(key.startswith('module.') for key in unet_state_dict.keys()):\n",
    "            print(\"[Load] Removing 'module.' prefix from UNet state_dict keys.\")\n",
    "            unet_state_dict = {k.partition('module.')[2]: v for k, v in unet_state_dict.items()}\n",
    "\n",
    "        unet.load_state_dict(unet_state_dict)\n",
    "        unet = unet.to(DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "        unet.eval()\n",
    "        print(f\"[Load] UNet Loaded to {DEVICE} ({INFERENCE_DTYPE}).\") # Corrected print format\n",
    "    except Exception as e:\n",
    "        print(f\"[Load Error] Failed to load UNet: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- 3. Load Action Conditioner ---\n",
    "    print(f\"\\n[Load] Initializing ActionConditioner...\")\n",
    "    try:\n",
    "        action_conditioner = ActionConditioningModule(\n",
    "            target_token_dim=unet_cross_attention_dim, # Use actual dim from loaded UNet\n",
    "            freeze_clip=FREEZE_CLIP_IN_ACTION_MODULE\n",
    "        )\n",
    "\n",
    "        # Construct the correct checkpoint filename\n",
    "        action_cond_filename = f\"action_conditioner_epoch_{epoch}.pth\"\n",
    "        action_cond_path = os.path.join(checkpoint_dir, action_cond_filename)\n",
    "        action_cond_path_abs = os.path.abspath(action_cond_path)\n",
    "\n",
    "        if not os.path.exists(action_cond_path_abs):\n",
    "            raise FileNotFoundError(f\"ActionConditioner checkpoint not found at: {action_cond_path_abs}\")\n",
    "\n",
    "        print(f\"[Load] Loading ActionConditioner weights from: {action_cond_path_abs}\")\n",
    "        action_cond_state_dict = torch.load(action_cond_path_abs, map_location='cpu')\n",
    "\n",
    "        # Handle potential 'module.' prefix\n",
    "        if all(key.startswith('module.') for key in action_cond_state_dict.keys()):\n",
    "            print(\"[Load] Removing 'module.' prefix from ActionConditioner state_dict keys.\")\n",
    "            action_cond_state_dict = {k.partition('module.')[2]: v for k, v in action_cond_state_dict.items()}\n",
    "\n",
    "        action_conditioner.load_state_dict(action_cond_state_dict)\n",
    "        action_conditioner = action_conditioner.to(DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "        action_conditioner.eval()\n",
    "        print(f\"[Load] ActionConditioner Loaded to {DEVICE} ({INFERENCE_DTYPE}).\") # Corrected print format\n",
    "    except Exception as e:\n",
    "        print(f\"[Load Error] Failed to load ActionConditioner: {e}\")\n",
    "        raise e\n",
    "\n",
    "    print(\"\\n--- Model Loading Complete ---\")\n",
    "    return vae, unet, action_conditioner\n",
    "\n",
    "# ==============================================================================\n",
    "#                     Image Processing Functions\n",
    "# ==============================================================================\n",
    "# Preprocessing is no longer needed for start image, but decode is essential\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_latent_to_pil(latents, vae):\n",
    "    \"\"\"Decodes latents (BCHW) to a PIL Image.\"\"\"\n",
    "    latents = latents.to(dtype=vae.dtype) # Match VAE dtype for decode\n",
    "    # Scale latent before VAE decode\n",
    "    latents = latents / VAE_SCALE_FACTOR\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1) # Denormalize [-1, 1] -> [0, 1]\n",
    "\n",
    "    # Handle batch size > 1 if necessary, return first image\n",
    "    if image.shape[0] > 1:\n",
    "        print(f\"Warning: Decoding batch of {image.shape[0]}, returning only the first image.\")\n",
    "    image = image[0].cpu().permute(1, 2, 0).float().numpy() # CHW -> HWC, float32 numpy\n",
    "    image = (image * 255).round().astype(np.uint8)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "# ==============================================================================\n",
    "#                     Core Prediction Function\n",
    "# ==============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next_frame(current_latent, action_string, unet, action_conditioner):\n",
    "    \"\"\"\n",
    "    Predicts the latent representation of the next frame using the trained models.\n",
    "    Input: current_latent (BCHW tensor), action_string (str or list/tuple of strings)\n",
    "    Output: predicted_next_latent (BCHW tensor)\n",
    "    \"\"\"\n",
    "    unet.eval()\n",
    "    action_conditioner.eval()\n",
    "\n",
    "    # 1. Get Action Embedding\n",
    "    if isinstance(action_string, str):\n",
    "        action_string = [action_string] # Ensure it's a list for the conditioner\n",
    "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "\n",
    "    # 2. Prepare UNet inputs\n",
    "    # Timestep: Use a low timestep (e.g., 1) for inference.\n",
    "    # Training added noise based on timesteps up to MAX_NOISE_STEP_SAMPLE.\n",
    "    # For inference, we input the *current* state (assumed low noise) and ask for the next state.\n",
    "    # The timestep tells the UNet the assumed noise level of the input.\n",
    "    timestep = torch.tensor([1], device=DEVICE).long() # Using t=1 (or try t=0)\n",
    "    bsz = current_latent.shape[0]\n",
    "    if bsz != 1: # Repeat timestep if batch size is > 1 (though usually 1 for interactive)\n",
    "         timestep = timestep.repeat(bsz)\n",
    "\n",
    "    # 3. UNet Prediction\n",
    "    predicted_next_latent = unet(\n",
    "        sample=current_latent.to(dtype=INFERENCE_DTYPE), # Input current state\n",
    "        timestep=timestep,\n",
    "        encoder_hidden_states=action_emb\n",
    "    ).sample # Output is the predicted next state latent\n",
    "\n",
    "    return predicted_next_latent\n",
    "\n",
    "# ==============================================================================\n",
    "#                       Inference Execution (Modified Start)\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\": # Ensures this runs only when executed directly\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Using inference dtype: {INFERENCE_DTYPE}\")\n",
    "\n",
    "    # --- 1. Load Models ---\n",
    "    try:\n",
    "        vae, unet, action_conditioner = load_trained_models(\n",
    "            CHECKPOINT_DIR, TARGET_EPOCH, SD_MODEL_ID, VAE_PATH\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n[FATAL ERROR] Could not find necessary checkpoint files.\")\n",
    "        print(e)\n",
    "        print(\"\\nPlease ensure CHECKPOINT_DIR and TARGET_EPOCH are correct and the files exist:\")\n",
    "        print(f\"  - {os.path.join(CHECKPOINT_DIR, f'unet_epoch_{TARGET_EPOCH}.pth')}\")\n",
    "        print(f\"  - {os.path.join(CHECKPOINT_DIR, f'action_conditioner_epoch_{TARGET_EPOCH}.pth')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FATAL ERROR] An unexpected error occurred during model loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    else: # Proceed only if models loaded successfully\n",
    "\n",
    "        # --- 2. Initialize Latent State ---\n",
    "        print(\"\\n--- Initializing Start Latent State (Zeros) ---\")\n",
    "        # Create a starting latent tensor of zeros (batch size 1)\n",
    "        # Shape: (batch_size, channels, height, width)\n",
    "        current_latent = torch.zeros(\n",
    "            (1, LATENT_CHANNELS, LATENT_HEIGHT, LATENT_WIDTH),\n",
    "            device=DEVICE,\n",
    "            dtype=INFERENCE_DTYPE\n",
    "        )\n",
    "        print(f\"Initial latent created with shape: {current_latent.shape}\")\n",
    "\n",
    "        # --- 3. Interactive Generation Loop ---\n",
    "        generated_frames_pil = []\n",
    "        # No initial frame to add or display\n",
    "\n",
    "        print(\"\\n--- Starting Interactive Generation ---\")\n",
    "        for i in tqdm(range(NUM_INFERENCE_STEPS), desc=\"Generating Frames\"):\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Step {i+1}/{NUM_INFERENCE_STEPS}\")\n",
    "\n",
    "            # Get action from user\n",
    "            action = input(f\"Enter action string for step {i+1} [or press Enter to stop]: \")\n",
    "            if not action.strip():\n",
    "                print(\"Empty action entered, stopping generation.\")\n",
    "                break\n",
    "\n",
    "            # Predict the next latent state based on the CURRENT latent and action\n",
    "            print(f\"Predicting frame {i+1} for action: '{action}'...\")\n",
    "            try:\n",
    "                predicted_latent = predict_next_frame(\n",
    "                    current_latent, action, unet, action_conditioner\n",
    "                )\n",
    "\n",
    "                # Decode the predicted latent to an image\n",
    "                predicted_pil_image = decode_latent_to_pil(predicted_latent, vae)\n",
    "                generated_frames_pil.append(predicted_pil_image)\n",
    "\n",
    "                # Display the result\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.imshow(predicted_pil_image)\n",
    "                plt.title(f\"Generated Frame {i+1} - Action: '{action}'\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "                # Update current latent for the next step\n",
    "                current_latent = predicted_latent\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n[ERROR during prediction/decoding step {i+1}]\")\n",
    "                print(e)\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                print(\"Skipping to next step if possible...\")\n",
    "                continue # Try to continue if one step fails\n",
    "\n",
    "\n",
    "        print(\"\\n--- Inference Loop Finished ---\")\n",
    "\n",
    "        # Optional: Display all generated frames together at the end\n",
    "        if generated_frames_pil: # Check if any frames were generated\n",
    "             print(\"\\nDisplaying Generated Sequence:\")\n",
    "             num_frames = len(generated_frames_pil)\n",
    "             cols = min(num_frames, 5) # Display max 5 frames per row\n",
    "             rows = (num_frames + cols - 1) // cols\n",
    "             fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "             # Ensure axes is always iterable, even if rows*cols=1\n",
    "             if rows * cols == 1:\n",
    "                 axes = np.array([axes])\n",
    "             axes = axes.flatten() # Make axes array 1D for easy iteration\n",
    "\n",
    "             for idx, frame in enumerate(generated_frames_pil):\n",
    "                 axes[idx].imshow(frame)\n",
    "                 axes[idx].set_title(f\"Frame {idx+1}\") # Start numbering from 1\n",
    "                 axes[idx].axis('off')\n",
    "             # Hide any unused subplots\n",
    "             for idx in range(num_frames, len(axes)):\n",
    "                  axes[idx].axis('off')\n",
    "             plt.tight_layout()\n",
    "             plt.show()\n",
    "        else:\n",
    "             print(\"No frames were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using inference dtype: torch.float32\n",
      "\n",
      "--- Loading Models: Epoch 10 from './training_diffusion/' ---\n",
      "[Load] Initializing VAE from runwayml/stable-diffusion-v1-5...\n",
      "[Load] Loading custom VAE weights from: /Users/chinmaysultanpuri/Desktop/GenAI/models/vae_full_finetuned_stage2_v10_api_fix_corrected.pth\n",
      "[Load] Custom VAE weights applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] VAE Loaded to mps (torch.float32).\n",
      "\n",
      "[Load] Initializing UNet from runwayml/stable-diffusion-v1-5...\n",
      "[Load] Base UNet loaded. Cross Attn Dim: 768\n",
      "[Load] Loading UNet weights from: /Users/chinmaysultanpuri/Desktop/GenAI/training_diffusion/unet_epoch_10.pth\n",
      "[Load] UNet Loaded to mps (torch.float32).\n",
      "\n",
      "[Load] Initializing ActionConditioner...\n",
      "[ActionConditioner] Initializing with CLIP: openai/clip-vit-large-patch14\n",
      "[ActionConditioner]   CLIP output dim: 768\n",
      "[ActionConditioner]   Freezing CLIP text model parameters.\n",
      "[ActionConditioner]   Added projection layers (target dim 768 might update)\n",
      "[Load] Loading ActionConditioner weights from: /Users/chinmaysultanpuri/Desktop/GenAI/training_diffusion/action_conditioner_epoch_10.pth\n",
      "[Load] ActionConditioner Loaded to mps (torch.float32).\n",
      "\n",
      "--- Model Loading Complete ---\n",
      "\n",
      "--- Initializing Start Latent State (Random Game of Life) ---\n",
      "Encoded initial Game of Life frame to latent with shape: torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Automated Generation with Action 'A' ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad35dc70456e4bcfa3d6bc1b4a933c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Frames:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Step 1/10\n",
      "Predicting frame 1 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 1]\n",
      "MPS backend out of memory (MPS allocated: 8.24 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 5.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 2/10\n",
      "Predicting frame 2 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 2]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 317, in predict_next_frame\n",
      "    predicted_next_latent = unet(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/diffusers/models/unets/unet_2d_condition.py\", line 1214, in forward\n",
      "    sample, res_samples = downsample_block(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/diffusers/models/unets/unet_2d_blocks.py\", line 1270, in forward\n",
      "    hidden_states = attn(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/diffusers/models/transformers/transformer_2d.py\", line 427, in forward\n",
      "    hidden_states = block(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention.py\", line 514, in forward\n",
      "    attn_output = self.attn1(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention_processor.py\", line 605, in forward\n",
      "    return self.processor(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/diffusers/models/attention_processor.py\", line 3317, in __call__\n",
      "    hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.24 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 5.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 3/10\n",
      "Predicting frame 3 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 3]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 4/10\n",
      "Predicting frame 4 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 4]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 5/10\n",
      "Predicting frame 5 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 5]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 6/10\n",
      "Predicting frame 6 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 6]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 7/10\n",
      "Predicting frame 7 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 7]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 8/10\n",
      "Predicting frame 8 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 8]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 9/10\n",
      "Predicting frame 9 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 9]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "------------------------------\n",
      "Step 10/10\n",
      "Predicting frame 10 for action: 'A'...\n",
      "\n",
      "[ERROR during prediction/decoding step 10]\n",
      "MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Skipping to next step if possible...\n",
      "\n",
      "--- Inference Loop Finished ---\n",
      "No frames were generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 386, in <module>\n",
      "    predicted_latent = predict_next_frame(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 308, in predict_next_frame\n",
      "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/var/folders/_k/vbkl_k7n1v132xrv3mf0f83r0000gn/T/ipykernel_7501/4232640228.py\", line 99, in forward\n",
      "    clip_inputs = self.clip_tokenizer(\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 820, in to\n",
      "    self.data = {\n",
      "  File \"/Users/chinmaysultanpuri/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py\", line 821, in <dictcomp>\n",
      "    k: v.to(device=device, non_blocking=non_blocking) if isinstance(v, torch.Tensor) else v\n",
      "RuntimeError: MPS backend out of memory (MPS allocated: 8.10 GB, other allocations: 1.00 GB, max allowed: 9.07 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#                            Imports\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm # Use notebook version for better display\n",
    "\n",
    "# Diffusers & Transformers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# Disable noisy warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' # Suppress TensorFlow oneDNN warnings if TF is installed\n",
    "\n",
    "# Game of Life Specific Imports\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# ==============================================================================\n",
    "#                      Configuration (MUST MATCH TRAINING)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Core Model & Paths ---\n",
    "SD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-large-patch14\"\n",
    "CHECKPOINT_DIR = \"./training_diffusion/\"\n",
    "TARGET_EPOCH = 10\n",
    "VAE_PATH = \"./models/vae_full_finetuned_stage2_v10_api_fix_corrected.pth\"\n",
    "\n",
    "# --- Image & Latent Dimensions (Must match training) ---\n",
    "IMAGE_RESOLUTION = 512\n",
    "VAE_SCALE_FACTOR = 0.18215\n",
    "LATENT_CHANNELS = 4\n",
    "VAE_DOWNSAMPLE_FACTOR = 8\n",
    "LATENT_HEIGHT = IMAGE_RESOLUTION // VAE_DOWNSAMPLE_FACTOR\n",
    "LATENT_WIDTH = IMAGE_RESOLUTION // VAE_DOWNSAMPLE_FACTOR\n",
    "\n",
    "# --- Action Conditioning Module Config (Must match training) ---\n",
    "CLIP_MAX_LENGTH = 16\n",
    "FREEZE_CLIP_IN_ACTION_MODULE = True\n",
    "\n",
    "# --- Inference Specific Config ---\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "INFERENCE_DTYPE = torch.float32\n",
    "NUM_INFERENCE_STEPS = 10\n",
    "\n",
    "# --- Game of Life Parameters ---\n",
    "GRID_SIZE = (64, 64)\n",
    "INITIAL_ALIVE_PROB = 0.25\n",
    "\n",
    "# Action to use for all steps.  Modify this for different effects.\n",
    "ACTION_STRING = \"A\"\n",
    "\n",
    "# ==============================================================================\n",
    "#              Model Definitions (Copy EXACTLY from training)\n",
    "# ==============================================================================\n",
    "\n",
    "class ActionConditioningModule(nn.Module):\n",
    "    \"\"\" Encodes action strings using CLIP and projects to target dimension. \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        clip_model_name: str = CLIP_MODEL_NAME,\n",
    "        max_clip_length: int = CLIP_MAX_LENGTH,\n",
    "        target_token_dim: int = 768, # This will be updated based on UNet later\n",
    "        freeze_clip: bool = FREEZE_CLIP_IN_ACTION_MODULE\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.target_token_dim = target_token_dim\n",
    "        self.max_clip_length = max_clip_length\n",
    "        self.freeze_clip = freeze_clip\n",
    "        print(f\"[ActionConditioner] Initializing with CLIP: {clip_model_name}\")\n",
    "        self.clip_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.clip_text_model = CLIPTextModel.from_pretrained(clip_model_name)\n",
    "        clip_output_dim = self.clip_text_model.config.hidden_size\n",
    "        print(f\"[ActionConditioner]   CLIP output dim: {clip_output_dim}\")\n",
    "\n",
    "        if freeze_clip:\n",
    "            print(\"[ActionConditioner]   Freezing CLIP text model parameters.\")\n",
    "            for param in self.clip_text_model.parameters(): param.requires_grad = False\n",
    "            self.clip_text_model.eval() # Ensure it's in eval mode if frozen\n",
    "        else:\n",
    "            print(\"[ActionConditioner]   CLIP text model parameters are TRAINABLE (will be set to eval).\")\n",
    "            # We still set to eval for inference regardless\n",
    "\n",
    "        # Projection layers\n",
    "        self.action_projection1 = nn.Linear(clip_output_dim, clip_output_dim)\n",
    "        self.action_projection2 = nn.Linear(clip_output_dim, target_token_dim) # Target dim might change\n",
    "        print(f\"[ActionConditioner]   Added projection layers (target dim {target_token_dim} might update)\")\n",
    "\n",
    "    def forward(self, action_strings, device):\n",
    "        b = len(action_strings)\n",
    "        clip_inputs = self.clip_tokenizer(\n",
    "            action_strings, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_clip_length, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Ensure eval mode and no gradients during inference\n",
    "        self.clip_text_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.clip_text_model(**clip_inputs)\n",
    "            last_hidden = outputs.last_hidden_state # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "            # --- Using pooled output (like training) ---\n",
    "            eos_token_id = self.clip_tokenizer.eos_token_id\n",
    "            input_ids = clip_inputs[\"input_ids\"]\n",
    "            batch_size, seq_len = last_hidden.shape[0], last_hidden.shape[1]\n",
    "            eos_mask = (input_ids == eos_token_id)\n",
    "            eos_indices = torch.where(\n",
    "                eos_mask.any(dim=1),\n",
    "                eos_mask.max(dim=1).indices,\n",
    "                torch.tensor(seq_len - 1, device=device)\n",
    "            )\n",
    "            eos_indices = eos_indices.clamp(0, seq_len - 1)\n",
    "            pooled = last_hidden[torch.arange(batch_size, device=device), eos_indices]\n",
    "            # --- End Pooled Output ---\n",
    "\n",
    "        # Projection layers (still no grad)\n",
    "        with torch.no_grad():\n",
    "            x = F.silu(self.action_projection1(pooled))\n",
    "            x = self.action_projection2(x)\n",
    "\n",
    "        # Return shape [batch_size, 1, target_token_dim] for UNet cross-attention\n",
    "        return x.unsqueeze(1)\n",
    "\n",
    "# ==============================================================================\n",
    "#                     Game of Life Helper Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def initialize_grid(size, alive_prob=INITIAL_ALIVE_PROB):\n",
    "    \"\"\"Initializes a random CGoL grid.\"\"\"\n",
    "    return np.random.choice([0, 1], size=size, p=[1 - alive_prob, alive_prob])\n",
    "\n",
    "def step_grid(grid):\n",
    "    \"\"\"Performs one step of Conway's Game of Life using convolution.\"\"\"\n",
    "    kernel = np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    neighbor_counts = convolve2d(grid, kernel, mode='same', boundary='wrap')\n",
    "    birth = (grid == 0) & (neighbor_counts == 3)\n",
    "    survival = (grid == 1) & ((neighbor_counts == 2) | (neighbor_counts == 3))\n",
    "    new_grid = np.zeros_like(grid)\n",
    "    new_grid[birth | survival] = 1\n",
    "    return new_grid\n",
    "\n",
    "def render_grid_to_image(grid, image_resolution=IMAGE_RESOLUTION):\n",
    "    \"\"\"Renders the CGoL grid (0/1) to a PIL Image.\"\"\"\n",
    "    grid_h, grid_w = grid.shape\n",
    "    scale_h = image_resolution // grid_h\n",
    "    scale_w = image_resolution // grid_w\n",
    "    black = np.array([0, 0, 0], dtype=np.uint8)\n",
    "    white = np.array([255, 255, 255], dtype=np.uint8)\n",
    "    bool_grid = grid.astype(bool)\n",
    "    colored_grid = np.zeros((grid_h, grid_w, 3), dtype=np.uint8)\n",
    "    colored_grid[~bool_grid] = black\n",
    "    colored_grid[bool_grid] = white\n",
    "    upscaled_frame_np = colored_grid.repeat(scale_h, axis=0).repeat(scale_w, axis=1)\n",
    "    image = Image.fromarray(upscaled_frame_np)\n",
    "    return image\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_image_to_latent(image, vae, device=DEVICE, dtype=INFERENCE_DTYPE):\n",
    "    \"\"\"Encodes a PIL Image to a latent tensor.\"\"\"\n",
    "    image = image.resize((IMAGE_RESOLUTION, IMAGE_RESOLUTION))  # Ensure correct size\n",
    "    image_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device).to(dtype) * 2 - 1 # Normalize to [-1, 1]\n",
    "    latent = vae.encode(image_tensor).latent_dist.sample() * VAE_SCALE_FACTOR # Scale\n",
    "    return latent\n",
    "\n",
    "# ==============================================================================\n",
    "#                        Loading Function (Corrected)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_trained_models(checkpoint_dir, epoch, sd_model_id, vae_path=None):\n",
    "    \"\"\"Loads the trained UNet, ActionConditioner, and VAE.\"\"\"\n",
    "    print(f\"\\n--- Loading Models: Epoch {epoch} from '{checkpoint_dir}' ---\")\n",
    "\n",
    "    # --- 1. Load VAE ---\n",
    "    print(f\"[Load] Initializing VAE from {sd_model_id}...\")\n",
    "    try:\n",
    "        vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder=\"vae\")\n",
    "        if vae_path and os.path.exists(vae_path):\n",
    "            abs_vae_path = os.path.abspath(vae_path)\n",
    "            print(f\"[Load] Loading custom VAE weights from: {abs_vae_path}\")\n",
    "            if os.path.exists(abs_vae_path):\n",
    "                 vae.load_state_dict(torch.load(abs_vae_path, map_location='cpu'), strict=False)\n",
    "                 print(\"[Load] Custom VAE weights applied.\")\n",
    "            else:\n",
    "                 print(f\"[Load Warning] Custom VAE path specified but not found at {abs_vae_path}. Using base VAE.\")\n",
    "        elif vae_path:\n",
    "             print(f\"[Load Warning] Custom VAE path '{vae_path}' specified but does not exist. Using base VAE.\")\n",
    "        else:\n",
    "             print(\"[Load] Using base VAE weights.\")\n",
    "\n",
    "        vae = vae.to(DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "        vae.eval()\n",
    "        print(f\"[Load] VAE Loaded to {DEVICE} ({INFERENCE_DTYPE}).\") # Corrected print format\n",
    "    except Exception as e:\n",
    "        print(f\"[Load Error] Failed to load VAE: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- 2. Load UNet ---\n",
    "    print(f\"\\n[Load] Initializing UNet from {sd_model_id}...\")\n",
    "    try:\n",
    "        unet = UNet2DConditionModel.from_pretrained(sd_model_id, subfolder=\"unet\")\n",
    "        unet_cross_attention_dim = unet.config.cross_attention_dim\n",
    "        print(f\"[Load] Base UNet loaded. Cross Attn Dim: {unet_cross_attention_dim}\")\n",
    "\n",
    "        # Construct the correct checkpoint filename\n",
    "        unet_filename = f\"unet_epoch_{epoch}.pth\"\n",
    "        unet_path = os.path.join(checkpoint_dir, unet_filename)\n",
    "        unet_path_abs = os.path.abspath(unet_path)\n",
    "\n",
    "        if not os.path.exists(unet_path_abs):\n",
    "            raise FileNotFoundError(f\"UNet checkpoint not found at expected path: {unet_path_abs}\")\n",
    "\n",
    "        print(f\"[Load] Loading UNet weights from: {unet_path_abs}\")\n",
    "        unet_state_dict = torch.load(unet_path_abs, map_location='cpu')\n",
    "\n",
    "        # Handle potential 'module.' prefix from DDP saving\n",
    "        if all(key.startswith('module.') for key in unet_state_dict.keys()):\n",
    "            print(\"[Load] Removing 'module.' prefix from UNet state_dict keys.\")\n",
    "            unet_state_dict = {k.partition('module.')[2]: v for k, v in unet_state_dict.items()}\n",
    "\n",
    "        unet.load_state_dict(unet_state_dict)\n",
    "        unet = unet.to(DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "        unet.eval()\n",
    "        print(f\"[Load] UNet Loaded to {DEVICE} ({INFERENCE_DTYPE}).\") # Corrected print format\n",
    "    except Exception as e:\n",
    "        print(f\"[Load Error] Failed to load UNet: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- 3. Load Action Conditioner ---\n",
    "    print(f\"\\n[Load] Initializing ActionConditioner...\")\n",
    "    try:\n",
    "        action_conditioner = ActionConditioningModule(\n",
    "            target_token_dim=unet_cross_attention_dim, # Use actual dim from loaded UNet\n",
    "            freeze_clip=FREEZE_CLIP_IN_ACTION_MODULE\n",
    "        )\n",
    "\n",
    "        # Construct the correct checkpoint filename\n",
    "        action_cond_filename = f\"action_conditioner_epoch_{epoch}.pth\"\n",
    "        action_cond_path = os.path.join(checkpoint_dir, action_cond_filename)\n",
    "        action_cond_path_abs = os.path.abspath(action_cond_path)\n",
    "\n",
    "        if not os.path.exists(action_cond_path_abs):\n",
    "            raise FileNotFoundError(f\"ActionConditioner checkpoint not found at: {action_cond_path_abs}\")\n",
    "\n",
    "        print(f\"[Load] Loading ActionConditioner weights from: {action_cond_path_abs}\")\n",
    "        action_cond_state_dict = torch.load(action_cond_path_abs, map_location='cpu')\n",
    "\n",
    "        # Handle potential 'module.' prefix\n",
    "        if all(key.startswith('module.') for key in action_cond_state_dict.keys()):\n",
    "            print(\"[Load] Removing 'module.' prefix from ActionConditioner state_dict keys.\")\n",
    "            action_cond_state_dict = {k.partition('module.')[2]: v for k, v in action_cond_state_dict.items()}\n",
    "\n",
    "        action_conditioner.load_state_dict(action_cond_state_dict)\n",
    "        action_conditioner = action_conditioner.to(DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "        action_conditioner.eval()\n",
    "        print(f\"[Load] ActionConditioner Loaded to {DEVICE} ({INFERENCE_DTYPE}).\") # Corrected print format\n",
    "    except Exception as e:\n",
    "        print(f\"[Load Error] Failed to load ActionConditioner: {e}\")\n",
    "        raise e\n",
    "\n",
    "    print(\"\\n--- Model Loading Complete ---\")\n",
    "    return vae, unet, action_conditioner\n",
    "\n",
    "# ==============================================================================\n",
    "#                     Image Processing Functions\n",
    "# ==============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_latent_to_pil(latents, vae):\n",
    "    \"\"\"Decodes latents (BCHW) to a PIL Image.\"\"\"\n",
    "    latents = latents.to(dtype=vae.dtype) # Match VAE dtype for decode\n",
    "    # Scale latent before VAE decode\n",
    "    latents = latents / VAE_SCALE_FACTOR\n",
    "    image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1) # Denormalize [-1, 1] -> [0, 1]\n",
    "\n",
    "    # Handle batch size > 1 if necessary, return first image\n",
    "    if image.shape[0] > 1:\n",
    "        print(f\"Warning: Decoding batch of {image.shape[0]}, returning only the first image.\")\n",
    "    image = image[0].cpu().permute(1, 2, 0).float().numpy() # CHW -> HWC, float32 numpy\n",
    "    image = (image * 255).round().astype(np.uint8)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "# ==============================================================================\n",
    "#                     Core Prediction Function\n",
    "# ==============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next_frame(current_latent, action_string, unet, action_conditioner):\n",
    "    \"\"\"\n",
    "    Predicts the latent representation of the next frame using the trained models.\n",
    "    Input: current_latent (BCHW tensor), action_string (str or list/tuple of strings)\n",
    "    Output: predicted_next_latent (BCHW tensor)\n",
    "    \"\"\"\n",
    "    unet.eval()\n",
    "    action_conditioner.eval()\n",
    "\n",
    "    # 1. Get Action Embedding\n",
    "    if isinstance(action_string, str):\n",
    "        action_string = [action_string] # Ensure it's a list for the conditioner\n",
    "    action_emb = action_conditioner(action_string, device=DEVICE).to(dtype=INFERENCE_DTYPE)\n",
    "\n",
    "    # 2. Prepare UNet inputs\n",
    "    timestep = torch.tensor([1], device=DEVICE).long() # Using t=1\n",
    "    bsz = current_latent.shape[0]\n",
    "    if bsz != 1:\n",
    "         timestep = timestep.repeat(bsz)\n",
    "\n",
    "    # 3. UNet Prediction\n",
    "    predicted_next_latent = unet(\n",
    "        sample=current_latent.to(dtype=INFERENCE_DTYPE), # Input current state\n",
    "        timestep=timestep,\n",
    "        encoder_hidden_states=action_emb\n",
    "    ).sample # Output is the predicted next state latent\n",
    "\n",
    "    return predicted_next_latent\n",
    "\n",
    "# ==============================================================================\n",
    "#                       Inference Execution (No Interactive Input)\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"Using inference dtype: {INFERENCE_DTYPE}\")\n",
    "\n",
    "    # --- 1. Load Models ---\n",
    "    try:\n",
    "        vae, unet, action_conditioner = load_trained_models(\n",
    "            CHECKPOINT_DIR, TARGET_EPOCH, SD_MODEL_ID, VAE_PATH\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n[FATAL ERROR] Could not find necessary checkpoint files.\")\n",
    "        print(e)\n",
    "        print(\"\\nPlease ensure CHECKPOINT_DIR and TARGET_EPOCH are correct and the files exist:\")\n",
    "        print(f\"  - {os.path.join(CHECKPOINT_DIR, f'unet_epoch_{TARGET_EPOCH}.pth')}\")\n",
    "        print(f\"  - {os.path.join(CHECKPOINT_DIR, f'action_conditioner_epoch_{TARGET_EPOCH}.pth')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FATAL ERROR] An unexpected error occurred during model loading: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    else: # Proceed only if models loaded successfully\n",
    "\n",
    "        # --- 2. Initialize Latent State (Random Game of Life) ---\n",
    "        print(\"\\n--- Initializing Start Latent State (Random Game of Life) ---\")\n",
    "        try:\n",
    "            # Generate random initial Game of Life frame\n",
    "            initial_grid = initialize_grid(GRID_SIZE)\n",
    "            initial_image = render_grid_to_image(initial_grid)\n",
    "\n",
    "            # Encode image to latent\n",
    "            current_latent = encode_image_to_latent(initial_image, vae)\n",
    "            print(f\"Encoded initial Game of Life frame to latent with shape: {current_latent.shape}\")\n",
    "\n",
    "            # Display the initial frame\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(initial_image)\n",
    "            plt.title(\"Initial Game of Life Frame\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR generating/encoding initial Game of Life frame]\")\n",
    "            print(e)\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            exit() # Exit if the initial frame fails.  It's critical.\n",
    "\n",
    "        # --- 3. Generation Loop (Automated with \"A\") ---\n",
    "        generated_frames_pil = []\n",
    "\n",
    "        print(\"\\n--- Starting Automated Generation with Action 'A' ---\")\n",
    "        for i in tqdm(range(NUM_INFERENCE_STEPS), desc=\"Generating Frames\"):\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Step {i+1}/{NUM_INFERENCE_STEPS}\")\n",
    "\n",
    "            # Predict the next latent state (always action \"A\")\n",
    "            print(f\"Predicting frame {i+1} for action: '{ACTION_STRING}'...\")\n",
    "            try:\n",
    "                predicted_latent = predict_next_frame(\n",
    "                    current_latent, ACTION_STRING, unet, action_conditioner\n",
    "                )\n",
    "\n",
    "                # Decode the predicted latent to an image\n",
    "                predicted_pil_image = decode_latent_to_pil(predicted_latent, vae)\n",
    "                generated_frames_pil.append(predicted_pil_image)\n",
    "\n",
    "                # Display the result\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.imshow(predicted_pil_image)\n",
    "                plt.title(f\"Generated Frame {i+1} - Action: '{ACTION_STRING}'\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "                # Update current latent for the next step\n",
    "                current_latent = predicted_latent\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n[ERROR during prediction/decoding step {i+1}]\")\n",
    "                print(e)\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                print(\"Skipping to next step if possible...\")\n",
    "                continue # Try to continue if one step fails\n",
    "\n",
    "        print(\"\\n--- Inference Loop Finished ---\")\n",
    "\n",
    "        # Optional: Display all generated frames together at the end\n",
    "        if generated_frames_pil: # Check if any frames were generated\n",
    "             print(\"\\nDisplaying Generated Sequence:\")\n",
    "             num_frames = len(generated_frames_pil)\n",
    "             cols = min(num_frames, 5) # Display max 5 frames per row\n",
    "             rows = (num_frames + cols - 1) // cols\n",
    "             fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "             # Ensure axes is always iterable, even if rows*cols=1\n",
    "             if rows * cols == 1:\n",
    "                 axes = np.array([axes])\n",
    "             axes = axes.flatten() # Make axes array 1D for easy iteration\n",
    "\n",
    "             for idx, frame in enumerate(generated_frames_pil):\n",
    "                 axes[idx].imshow(frame)\n",
    "                 axes[idx].set_title(f\"Frame {idx+1}\") # Start numbering from 1\n",
    "                 axes[idx].axis('off')\n",
    "             # Hide any unused subplots\n",
    "             for idx in range(num_frames, len(axes)):\n",
    "                  axes[idx].axis('off')\n",
    "             plt.tight_layout()\n",
    "             plt.show()\n",
    "        else:\n",
    "             print(\"No frames were generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
